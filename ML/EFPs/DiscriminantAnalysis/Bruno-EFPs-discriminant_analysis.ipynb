{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing data"
   ],
   "id": "c717a3dfbf625613"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T17:57:44.104490Z",
     "start_time": "2024-06-19T17:57:44.089119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# importando bibliotecas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA"
   ],
   "id": "a56fa8af75af9ba",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "# carregando base de dados\n",
    "# EFPg = np.load(\"DataEFPsJetTagging/EFP d5-connected-not_normalized/g_efps_d5_primed.npy\")\n",
    "# EFPq = np.load(\"DataEFPsJetTagging/EFP d5-connected-not_normalized/q_efps_d5_primed.npy\")\n",
    "# EFPt = np.load(\"DataEFPsJetTagging/EFP d5-connected-not_normalized/t_efps_d5_primed.npy\")"
   ],
   "id": "98d9e58d1adeed5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Visualization"
   ],
   "id": "652f6b1074532b11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:16:59.610069Z",
     "start_time": "2024-06-12T21:16:59.604554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = EFPg # escolhendo o conjunto de dados\n",
    "column_index = 40 # escolhendo o índice da coluna"
   ],
   "id": "b078042ac16d7b0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T21:16:59.658062Z",
     "start_time": "2024-06-12T21:16:59.612723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = dataset[:, column_index] # valores da coluna\n",
    "data_log = np.log(data) # logarithmic-scaling\n",
    "\n",
    "nbins = 50 # número de bins no histograma\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5), gridspec_kw={'width_ratios': [1, 1]})\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# histograma - sem normalizar\n",
    "axes[0].hist(data, bins=nbins, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_xlabel(\"Value\")\n",
    "axes[0].set_title(f'Histogram for Column {column_index} of the dataset')\n",
    "\n",
    "# histograma - normalizando\n",
    "axes[1].hist(data_log, bins=nbins, density=True, alpha=0.6, color='g', edgecolor='black')\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_xlabel(\"Value\")\n",
    "axes[1].set_title(f'Histogram for log(Column {column_index}) of the dataset')\n",
    "\n",
    "plt.show()"
   ],
   "id": "3ca7e96ac47a7f08",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 6\u001B[0m\n\u001B[0;32m      2\u001B[0m data_log \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(data) \u001B[38;5;66;03m# logarithmic-scaling\u001B[39;00m\n\u001B[0;32m      4\u001B[0m nbins \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m \u001B[38;5;66;03m# número de bins no histograma\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m fig, axes \u001B[38;5;241m=\u001B[39m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39msubplots(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m15\u001B[39m, \u001B[38;5;241m5\u001B[39m), gridspec_kw\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwidth_ratios\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m]})\n\u001B[0;32m      7\u001B[0m fig\u001B[38;5;241m.\u001B[39msubplots_adjust(wspace\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.3\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# histograma - sem normalizar\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Normalizations"
   ],
   "id": "7456afa0e33efe40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def norm(n,data):\n",
    "    # n = 0, 1, 2, 3, 4, 5\n",
    "    \n",
    "    if n==0:\n",
    "        return data\n",
    "    elif n==1:\n",
    "        return MinMaxScaler().fit_transform(data)\n",
    "    elif n==2:\n",
    "        return StandardScaler().fit_transform(data)\n",
    "    elif n==3:\n",
    "        return np.log(data)\n",
    "    elif n==4:\n",
    "        return MinMaxScaler().fit_transform(np.log(data))\n",
    "    else: # n==5\n",
    "        return StandardScaler().fit_transform(np.log(data))\n",
    "    \n",
    "# lembrando que não é possível aplicar MinMax/Standard e depois log, por isso não acrescentamos essas opções."
   ],
   "id": "2bea3cb95dcb8883",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# escolhendo a normalização\n",
    "n = 0"
   ],
   "id": "be047ea2604cb4e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Partitioning and Splitting"
   ],
   "id": "3e8f55950bd6aa1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split(tagging):\n",
    "    # tagging = \"top\" ou não (para quark/gluon tagging)\n",
    "    \n",
    "    # separação de variáveis preditoras (features) e alvo (target)\n",
    "    X_g,y_g = EFPg, np.zeros((EFPg.shape[0], 1))\n",
    "    X_q,y_q = EFPq, np.zeros((EFPq.shape[0], 1)) if tagging == \"top\" else np.ones((EFPq.shape[0], 1))\n",
    "    X_t,y_t = EFPt, np.ones((EFPt.shape[0], 1)) # não vai usar no caso tagging == qg\n",
    "    \n",
    "    # particionamento (amostragem estratificada)\n",
    "    X_g_train, X_g_test, y_g_train, y_g_test = train_test_split(X_g, y_g, test_size=0.2, random_state=1)\n",
    "    X_q_train, X_q_test, y_q_train, y_q_test = train_test_split(X_q, y_q, test_size=0.2, random_state=1)\n",
    "    X_t_train, X_t_test, y_t_train, y_t_test = train_test_split(X_t, y_t, test_size=0.2, random_state=1) if tagging == \"top\" else [np.empty((0, arr.shape[1])) for arr in [X_q_train, X_q_test, y_q_train, y_q_test]]\n",
    "\n",
    "    # concatenando g, q, t\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        np.concatenate((X_g_train, X_q_train, X_t_train), axis=0),\n",
    "        np.concatenate((X_g_test, X_q_test, X_t_test), axis=0),\n",
    "        np.concatenate((y_g_train, y_q_train, y_t_train), axis=0),\n",
    "        np.concatenate((y_g_test, y_q_test, y_t_test), axis=0))\n",
    "    \n",
    "    # normalização\n",
    "    X = np.concatenate((X_train, X_test), axis=0)\n",
    "    X = norm(n, X)\n",
    "    X_train, X_test = X[:len(X_train)], X[len(X_train):]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "id": "a32e17257542b6c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# escolhendo se vamos fazer top tagging ou quark/gluon tagging\n",
    "tagging = \"top\""
   ],
   "id": "4d5c328bab54c2ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = split(tagging)"
   ],
   "id": "b4a5c6acb8d3406f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sum((y_train == 1))"
   ],
   "id": "f72410efdb8bcad2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sum((y_train == 0))\n"
   ],
   "id": "f524564f91e66f7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dimensionality reduction"
   ],
   "id": "76bf59119ab9e226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# opções de conjuntos para aplicar o PCA:\n",
    "# X_train\n",
    "# X_test\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "explainability = pca.explained_variance_ratio_.cumsum()\n",
    "factors = range(1,len(pca.explained_variance_ratio_)+1)\n",
    "plt.scatter(factors,explainability)\n",
    "# plt.hlines(0.9,0,20,'r')\n",
    "plt.xlabel('Número de componentes')\n",
    "plt.ylabel('Explicabilidade dos dados')\n",
    "print('A explicabilidade da primeira componente principal (PC1) é:', explainability[0].round(3))"
   ],
   "id": "753cdd237e2f14e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# coeficientes da PC1\n",
    "pca.components_[0]"
   ],
   "id": "21f00509656664ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca.components_[1]"
   ],
   "id": "43b7a770a5a9d3c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_pca = pca.transform(X) # variáveis preditoras no espaço das PCs\n",
    "N = 20 # número de PCs que vamos pegar\n",
    "X_pca = X_pca[:,:N] # pegando as N primeiras PCs"
   ],
   "id": "56bf7d37b7970391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# executar essa célula para rodar o modelo utilizando apenas a PC1\n",
    "X_train, X_test = X_pca[:len(X_train)], X_pca[len(X_train):]"
   ],
   "id": "a187e0088d87cdcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "id": "ebe430d56ffe8613"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# escolhendo o modelo\n",
    "model = LinearDiscriminantAnalysis() # LinearDiscriminantAnalysis() ou QuadraticDiscriminantAnalysis(tol=1.0e-6)\n",
    "model.fit(X_train, y_train.ravel())"
   ],
   "id": "29da95db766c88b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NÃO EXECUTAR ESSA CÉLULA SE 0 MODELO NÃO POSSUIR intercept_ OU coef_\n",
    "\n",
    "# intercept\n",
    "print('Intercept:', model.intercept_[0])\n",
    "\n",
    "# weights\n",
    "coef = model.coef_[0]\n",
    "sns.barplot (x=np.arange(1, len(coef)+1), y=coef)\n",
    "plt.xlabel(\"EFPs\")\n",
    "plt.ylabel(\"Fitted weights\")\n",
    "plt.title(\"Weights - LinearDiscriminantAnalysis\");"
   ],
   "id": "1e21fbcdf10bbf35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "coef"
   ],
   "id": "88493edad2b3b976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Performance metrics"
   ],
   "id": "bf5b310df9af0176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def performance_metrics(set):\n",
    "    # set == \"train\", \"test\"\n",
    "    \n",
    "    # escolhendo se vamos avaliar o conjunto de treinamento ou de teste\n",
    "    if set == \"train\":\n",
    "        X_set, y_set = X_train, y_train\n",
    "    else: # set == \"test\"\n",
    "        X_set, y_set = X_test, y_test\n",
    "    \n",
    "    y_pred = model.predict(X_set)\n",
    "    # Classification Report\n",
    "    print('Classification Report:\\n',metrics.classification_report(y_set.ravel(), y_pred, digits=3))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix:\\n',metrics.confusion_matrix(y_set.ravel(), y_pred, normalize='all').round(2))\n",
    "    \n",
    "    y_predproba = model.predict_proba(X_set)[:, -1]\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_set.ravel(), y_predproba)\n",
    "    roc_auc = metrics.roc_auc_score(y_set.ravel(), y_predproba)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc)\n",
    "    display.plot()\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.show()"
   ],
   "id": "ab3f2cf2645b35d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "performance_metrics(\"train\")"
   ],
   "id": "4df31efe744ac767",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [],
   "id": "b9335d4ac2458d1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
